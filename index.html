<!doctype html>
<html lang="en">
    <head>
        <title>How Far is Video Generation from World Model: A Physical Law Perspective</title>

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <!-- Open Graph -->
        <meta property="og:url" content="https://phyworld.github.io/" />
        <!-- TODO: Modify it later -->
        <meta property="og:image" content="https://phyworld.github.io/static/img/preview.png" />
        <meta property="og:title" content="How Far is Video Generation from World Model: A Physical Law Perspective" />
        <meta property="og:description" content="We conduct a systematic study to investigate whether video generation is able to learn physical laws from videos, leveraging data and model scaling." />

        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
        <script src="./static/js/distill_template.v2.js"></script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>
            MathJax = {
                tex: {
                    inlineMath: [['$', '$']]
                }
            };
        </script>

        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>

        <script defer="" src="./static/js/hider.js"></script>
        <script src="./static/js/image_interact.js"></script>
        <script src="./static/js/switch_videos.js"></script>

        <link rel="stylesheet" href="./static/css/style.css">
        <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css">

        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
        <script defer src="./static/js/fontawesome.all.min.js"></script>


        <!-- medium zoom https://github.com/francoischalifour/medium-zoom -->
        <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script>  <!-- jquery -->
        <script defer src="./static/js/medium-zoom.min.js"></script>
        <script defer src="./static/js/zoom.js"></script>

        <style>
            /* Hide controls on hover for the specific video with class 'no-controls' */
            .no-controls::-webkit-media-controls {
                display: none !important;
            }
            .no-controls:hover::-webkit-media-controls {
                display: none !important;
            }
            /* Applying the same for Firefox */
            .no-controls::-moz-media-controls {
                display: none !important;
            }
            .no-controls:hover::-moz-media-controls {
                display: none !important;
            }
            #twitter_container {
                column-width: 250px;
                column-gap: 20px;
            }
            .table-container {
                margin: 20px 0;
                overflow-x: auto;
            }
            
            .table-container table {
                border-collapse: collapse;
                width: 100%;
                font-size: 0.9em;
            }
            
            .table-container th, .table-container td {
                border: 1px solid #ddd;
                padding: 8px;
                text-align: center;
            }
            
            .table-container th {
                background-color: #f5f5f5;
                font-weight: bold;
            }
            
            .table-container tr:hover {
                background-color: #f5f5f5;
            }
            
            .table-container caption {
                margin-bottom: 10px;
                text-align: left;
                font-weight: bold;
            }

            .data-table {
                width: 100%;
                border-collapse: collapse;
                margin: 20px 0;
            }

            .data-table th,
            .data-table td {
                padding: 12px 15px;
                border: 1px solid #ddd;
                text-align: center;
            }

            .data-table th {
                background-color: #f5f5f5;
                font-weight: bold;
            }

            .data-table tr:hover {
                background-color: #f5f5f5;
            }
        </style>
        </head>
        <body>
            <div class="header-wrapper">
                <div class="header-container" id="header-container">
                    <div class="header-content">
                        <h1 style="margin-top: 0px; font-size: xxx-large;">LangBridge</h1>
                        <h2 style="margin-top: 0px; font-size: xx-large;">Interpreting Image as a Combination</h2>
                        <h2 style="margin-top: 0px; font-size: xx-large;">of Language Embeddings</h2>
                        <h2 style="margin-top: 0px"><i>&mdash; A Novel Adapter for Vision-Language Alignment</i></h2>
                        <p>
                            We propose LangBridge, an interpretable vision-language adapter that grounds visual semantics in LLM language priors through linear combinations of vocabulary embeddings. Our contributions can be summarized as follows:
                        </p>
                        <div class="contributions">
                            <div class="contribution-item">
                                <h3>üîç Explainable Analysis for Visual-Language Alignment</h3>
                                <p>We provide a systematic study of the learning process of MLP adapters in Visual-Language alignment.</p>
                            </div>
                            <div class="contribution-item">
                                <h3>üîß Novel Adapter</h3>
                                <p>We introduce LangBridge, a novel adapter that transforms the visual features into visual embeddings by decomposing them into linear combinations of the LLM's vocabulary embeddings.</p>
                            </div>
                            <div class="contribution-item">
                                <h3>üîÑ Pretraining-Free Transfer</h3>
                                <p>LangBridge enables pre-training-free reuse between different large language models, significantly reducing the cost of pretraining.</p>
                            </div>
                        </div>
                        

    
                        <div class="button-container", style="text-align: center;">
                            <a href="https://arxiv.org/abs/[YourPaperID]" class="button paper-link" target="_blank">
                                <span class="icon is-small">
                                    <i class="ai ai-arxiv"></i>
                                </span>
                                arXiv
                            </a>
                            <a href="https://arxiv.org/pdf/[YourPaperID].pdf" class="button paper-link" target="_blank">
                                <span class="icon is-small">
                                    <i class="fas fa-file-pdf"></i>
                                </span>
                                <span>PDF</span>
                            </a>
                            <a href="https://github.com/OpenGVLab/LangBridge" class="button" target="_blank">
                                <span class="icon is-small">
                                    <i class="fab fa-github"></i>
                                </span>
                                <span>Code</span>
                            </a>
                            <a href="https://huggingface.co/datasets/OpenGVLab/LLaVA-CC3M-Pretrain" class="button" target="_blank">
                                <span class="icon is-small">
                                    <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face" style="height: 1em;">
                                </span>
                                <span>Weight</span>
                            </a>                        
                        </div>


                </div>
                <div class="header-image">
                    <img draggable="false" src="static/image/LangBridge_icon.jpg" class="teaser-image" width="100%">
                </div>
            </div>
        </div>
    
    <d-article>

        <div class="byline">
            <div class="byline-container">
                <div class="is-size-5 publication-authors">
                    <div style="margin-bottom: px;">
                        <span class="author-block">
                            <a href="#" class="author-link" target="_blank" style="color: darkgreen;">Jiaqi Liao<sup>*</sup><sup style="color:#6fbf73;">1</sup></a>,
                        </span>
                        <span class="author-block">
                            <a href="#" class="author-link" target="_blank" style="color: darkgreen;">Yuwei Niu<sup>*</sup><sup style="color:#ff0000;">6</sup><sup style="color:#0000ff;">8</sup></a>,
                        </span>
                        <span class="author-block">
                            <a href="#" class="author-link" target="_blank" style="color: darkgreen;">Fanqing Meng<sup>*</sup><sup style="color:#9b51e0;">5</sup><sup style="color:#6fbf73;">1</sup></a>,
                        </span>
                        <span class="author-block">
                            <a href="#" class="author-link" target="_blank" style="color: darkgreen;">Hao Li<sup style="color:#ffac33;">2</sup><sup style="color:#6fbf73;">1</sup></a>,
                        </span>
                        <span class="author-block">
                            <a href="#" class="author-link" target="_blank" style="color: darkgreen;">Changyao Tian<sup style="color:#ffac33;">2</sup><sup style="color:#6fbf73;">1</sup></a>,
                        </span>
                        <span class="author-block">
                            <a href="#" class="author-link" target="_blank" style="color: darkgreen;">Yinuo Du</a>
                        </span>
                    </div>
                    <div>
                        <span class="author-block">
                            <a href="#" class="author-link" target="_blank" style="color: darkgreen;">Yuwen Xiong<sup style="color:#6fbf73;">1</sup></a>,
                        </span>
                        <span class="author-block">
                            <a href="#" class="author-link" target="_blank" style="color: darkgreen;">Dianqi Li</a>,
                        </span>
                        <span class="author-block">
                            <a href="#" class="author-link" target="_blank" style="color: darkgreen;">Xizhou Zhu<sup style="color:#ed4b82;">3</sup><sup style="color:#007bff;">4</sup></a>,
                        </span>
                        <span class="author-block">
                            <a href="#" class="author-link" target="_blank" style="color: darkgreen;">Li Yuan<sup style="color:#ff0000;">6</sup><sup style="color:#00ff00;">7</sup></a>,
                        </span>
                        <span class="author-block">
                            <a href="#" class="author-link" target="_blank" style="color: darkgreen;">Jifeng Dai<sup style="color:#ed4b82;">3</sup><sup style="color:#6fbf73;">1</sup><sup>‚úâ</sup></a>,
                        </span>
                        <span class="author-block">
                            <a href="#" class="author-link" target="_blank" style="color: darkgreen;">Yu Cheng<sup style="color:#ffac33;">2</sup><sup>‚úâ</sup></a>
                        </span>
                    </div>
                </div>
        
                <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup style="color:#6fbf73;">1</sup>OpenGVLab, Shanghai AI Laboratory,</span>
                    <span class="author-block"><sup style="color:#ffac33;">2</sup>The Chinese University of Hong Kong,</span><br>
                    <span class="author-block"><sup style="color:#ed4b82;">3</sup>Tsinghua University,</span>
                    <span class="author-block"><sup style="color:#007bff;">4</sup>SenseTime Research,</span>
                    <span class="author-block"><sup style="color:#9b51e0;">5</sup>Shanghai Jiao Tong University,</span><br>
                    <span class="author-block"><sup style="color:#ff0000;">6</sup>Peking University,</span>
                    <span class="author-block"><sup style="color:#00ff00;">7</sup>PengCheng Laboratory,</span>
                    <span class="author-block"><sup style="color:#0000ff;">8</sup>Chongqing University</span>
                </div>
        
                <div class="is-size-5 publication-authors">
                    <span class="author-note" style="color: black;">*Equal contribution</span>
                </div>
            </div>
        </div>


        <p class="text abstract" style="margin-top: 5%;">
            <span style="color: blue;"><strong>Background</strong></span><br>
            Recent years have witnessed remarkable advances in Large Vision-Language Models (LVLMs), which have achieved human-level performance across various complex vision-language tasks. Following LLaVA's paradigm, mainstream LVLMs typically employ a shallow MLP for visual-language alignment through a two-stage training process: pretraining for cross-modal alignment followed by instruction tuning. While this approach has proven effective, the underlying mechanisms of how MLPs bridge the modality gap remain poorly understood. Although some research has explored how LLMs process transformed visual tokens, few studies have investigated the fundamental alignment mechanism. Furthermore, the MLP adapter requires retraining whenever switching LLM backbones.
        </p>

        <p class="text abstract" style="margin-top: 3%;">
            <span style="color: blue;"><strong>Contribution</strong></span><br>
            We propose LangBridge, an interpretable vision-language adapter that grounds visual semantics in LLM language priors through linear combinations of vocabulary embeddings. Our contributions can be summarized as follows:
            <ul class="text" style="margin-top: 0%; padding-left: 10%;">
                <li style="margin-bottom: 0;">1. <a href="#explainable_analysis"><strong>Explainable Analysis for Visual-Language Alignment</strong></a>: We first investigate the working principles of MLP adapters and discover that they learn to project visual embeddings into subspaces spanned by corresponding text embeddings progressively.</li>
                <li style="margin-bottom: 0;">2. <a href="#novel_adapter"><strong>Novel Adapter</strong></a>: Based on this insight, we propose LangBridge, a novel adapter that explicitly maps visual tokens to linear combinations of LLM vocabulary embeddings.</li>
                <li style="margin-bottom: 0;">3. <a href="#pretraining_free"><strong>Pretraining-Free Transfer</strong></a>: This innovative design enables pretraining-free adapter transfer across different LLMs while maintaining performance.</li>
            </ul>
        </p>

        <p class="text abstract" style="margin-top: 3%;">
            <span style="color: blue;"><strong><a href="#validation">Validation</a></strong></span><br>
            Our experimental results demonstrate that a LangBridge adapter pre-trained on Qwen2-0.5B can be directly applied to larger models such as LLaMA3-8B or Qwen2.5-14B with minimal performance degradation. Overall, LangBridge enables interpretable vision-language alignment by grounding visual semantics in LLM language priors, while its plug-and-play design ensures efficient reuse across multiple LLMs with minimal performance loss.
        </p>

        <div id="explainable_analysis" class="section">
            <h1 class="text">Explainable Analysis for Visual-Language Alignment</h1>

            <d-figure id="fig-explainable">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/image/Explainable_figure_2_00.jpg" alt="Explainable Analysis Figure" width="100%">
                    <figcaption>
                        <strong>Figure 1:</strong> <strong>Progressive semantic alignment in MLP adapters across training stages.</strong> Circular graphs demonstrate the evolution of visual-text token associations through four training phases (Pretrain-100, Pretrain-1000, Pretrain-2000, and Final-SFT).
                    </figcaption>
                </figure>
            </d-figure>
            
            <p class="text">
                In this section, we conduct a comprehensive analysis of how MLPs bridge the modality gap through two aspects:
            </p>
            
            <ul class="text">
                <li><a href="#semantic_analysis"><strong>Investigating how visual semantics are encoded in transformed embeddings</strong></a></li>
                <li><a href="#cross_modal_learning"><strong>Exploring how MLPs learn cross-modal alignment during training</strong></a></li>
            </ul>
        
            <h3 id="semantic_analysis" class="text">Semantic Analysis of Visual Embeddings</h3>
            <p class="text">
                To understand how visual semantics are encoded in transformed embeddings, we analyze the relationship between visual embeddings and text embeddings through cosine similarity: As shown in <span class="figure-ref">Figure 1</span>, visual embeddings exhibit strong correlations with semantically related text tokens. For example, image patches of red apples show high similarity to tokens like 'red' and 'Apple'. Moreover, some image tokens convey global information, for instance, '5,' which refers to the number of apples. This result demonstrates that visual embeddings are within the LLM's text embedding space through the MLP mapping, and are particularly close to their semantically related text embeddings.
            </p>
        
            <h3 id="cross_modal_learning" class="text">Cross-Modal Alignment Learning Process</h3>
            <p class="text">
                To investigate the development of cross-modal alignment capabilities, we conduct a temporal analysis across different training stages:
            </p>
            
            <ul class="text">
                <li>Pretrain-100 steps</li>
                <li>Pretrain-1000 steps</li>
                <li>Pretrain-2000 steps</li>
                <li>Final SFT</li>
            </ul>
            
            <p class="text">
                Our analysis reveals that MLPs undergo a progressive learning process in which they:
            </p>
            
            <ul class="text">
                <li>Initially establish basic visual-textual correspondences</li>
                <li>Gradually refine the projection of visual features into relevant text embedding subspaces</li>
                <li>Finally achieve robust semantic alignment between modalities</li>
            </ul>
        
            <p class="text">
                This evolutionary process is clearly demonstrated in <span class="figure-ref">Figure 1</span>, where we observe increasing semantic alignment strength and precision as training progresses. These results support our hypothesis that MLPs gradually develop their projection capabilities, incrementally learning to map visual features into regions close to the subspace spanned by their corresponding text embeddings. Building upon this insight, we introduce an explicit transformation approach called <strong>Language Basis Vector Projection</strong>. This approach explicitly projects visual features into the text embedding subspace of the LLM by representing them as linear combinations of the LLM's vocabulary embeddings.
            </p>

        </div>

        <div id="law_discovery" class="section">
            <h1 class="text">Method</h1>

            <d-figure id="figure-2">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/image/pipeline_new_00.jpg" alt="Explainable Analysis Figure" width="100%">
                    <figcaption>
                        <strong>Figure 2:</strong> <strong>Overview of LangBridge architecture and workflow.</strong>
                    </figcaption>
                </figure>
            </d-figure>
            
            <p class="text">
                In this section, we introduce LangBridge, a novel adapter that translates visual features into the language model's embedding space based on the Language Basis Vector Projection Theory. Our key insight is to decompose visual features into linear combinations of the LLM's vocabulary embeddings, enabling seamless integration with different LLM architectures. First, we present the core architecture of LangBridge (Section 3.1), which operates by extracting visual features from images and projecting them into probability distributions over the LLM's vocabulary space. These distributions are then used to construct visual tokens as weighted combinations of text embeddings. Finally, we explain how the pretrained LangBridge can be seamlessly reused across different LLM architectures without retraining (<a href="#pretraining_free">Section 3.2</a>).
            </p>
            
            <h2 id="novel_adapter" class="text">Novel Adapter</h2>
            <p class="text"> <strong>Architecture of LangBridge</strong><br> We present the architecture of LangBridge, a novel
                vision-language adapter based on the Language Basis Vector Projection Theory. As illustrated in <a href="#figure-2">Figure 2</a>, the
                framework operates through three stages: </p>
            <p class="text"> <strong>Stage 1: Visual Feature Extraction.</strong><br> 
                Given an input image \(\mathcal{I} \in \mathbb{R}^{H \times W \times 3}\), we employ a Vision Transformer to extract patch-aligned visual features: 
                \[\{v_i\}_{i=1}^N = \text{ViT}(\mathcal{I}), \quad v_i \in \mathbb{R}^D \tag{1}\] 
                where \(N = \frac{HW}{P^2}\) denotes the number of image patches with patch size \(P\). 
            </p>
            <p class="text"> <strong>Stage 2: Probability Computation.</strong><br> 
                After acquiring the visual features, we transform them into visual tokens and align them with the LLM's text embedding space. This process is carried out by
                decomposing the visual features into linear combinations of the LLM's vocabulary embeddings. We first apply a two-layer MLP to project the visual features into the LLM's text embedding
                space. Let \(\mathbf{v}\) represent the visual feature. The projection is given by: 
                \[\mathbf{v}_{\text{proj}} = \text{MLP}(\mathbf{v}) \tag{2}\] 
                where \(\mathbf{v}_{\text{proj}} \in \mathbb{R}^D\), and \(D\) is the dimensionality of
                the text embeddings. Next, we append a linear layer \(\mathbf{W} \in \mathbb{R}^{T \times D}\), where \(T\) is the
                vocabulary size of the LLM, to produce the probability distribution \(\mathbf{p}\), matching the LLM's vocab size:
                \[\mathbf{p} = \mathbf{W} \cdot \text{MLP}(\mathbf{v}) \tag{3}\]
            </p>
            <p class="text"> <strong>Stage 3: Linear Combination of Text Embeddings.</strong><br> 
                Once we have the probability distribution \(\mathbf{p}\) from Equation (3), the visual features are represented as a weighted combination of the LLM's
                text embeddings. The probability distribution \(\mathbf{p}\) serves as the coefficients for a linear combination of
                the LLM's vocabulary embeddings \(\mathbf{e}_i\), where each \(\mathbf{e}_i \in \mathbb{R}^D\) is a text embedding of the
                \(i\)-th token in the vocabulary. Specifically, the visual tokens \(\mathbf{v}_{\text{tokens}}\) are obtained as
                follows: 
                \[\mathbf{v}_{\text{tokens}} = \sum_{i=1}^T p_i \mathbf{e}_i \tag{4}\] 
                where \(p_i\) is the \(i\)-th component of the probability distribution \(\mathbf{p}\), and \(T\) is the vocabulary size of the LLM. 
                The resulting vector \(\mathbf{v}_{\text{tokens}} \in \mathbb{R}^D\) represents the final visual token, which aligns with the LLM's text
                embedding space. 
            </p>

            <h2 id="pretraining_free" class="text">Pretraining-Free Transfer</h2>
            <p class="text">
                A key feature of LangBridge is its ability to be seamlessly reused across different LLM architectures after being pretrained. As illustrated in <a href="#figure-2">Figure 2</a>, this reuse mechanism consists of two stages:
            </p>

            <p class="text">
                <strong>Stage 1: Pretrain.</strong><br>
                During the pretraining stage, we first extract image features through the Vision Encoder, then use LangBridge to learn to map these features to the space of Shared Vocab Embedding of LLM-1, optimizing by the caption loss. In this stage, LangBridge outputs probability distributions over the selected vocabulary.
            </p>

            <p class="text">
                <strong>Stage 2: SFT.</strong><br>
                When constructing a LVLM based on the new LLM-2, we do not need to retrain LangBridge in stage 1. Instead, we can directly reuse the adapters that were co-trained with LLM-1 in stage 1. The core principle behind this is that LangBridge only needs to output the probability distributions over the corresponding vocab instead of direct transform. When reusing LangBridge with different LLMs, such as Qwen2-0.5B and LLaMA3-8B, the key difficulty lies in the vocabulary (due to differences in vocabulary size). To enable reuse across different LLMs, as described in <a href="#figure-2">Figure 2</a>, we construct a shared vocabulary \(V_{shared}\). The reusing process can be expressed as:
            </p>

            <p class="text" style="text-align: center;">
                \[P = \text{LangBridge}_{\text{LLM}_1}(I) \in \mathbb{R}^{|V_{\text{shared}}|}\]
                \[\text{Visiontoken}_{\text{LLM}2} = P \cdot V_{\text{shared}}\]
            </p>

            <p class="text">
                Here, \(P\) represents the probability distribution over the shared vocabulary \(V_{\text{shared}}\). The operation \(P \cdot V_{\text{shared}}\) projects this distribution onto \(\text{LLM-2}\)'s vocabulary, resulting in the vision token \(\text{Visiontoken}_{\text{LLM-2}}\), which is then sent to \(\text{LLM-2}\) for processing.
            </p>

        </div>

        <div id="law_discovery" class="section">
            <h1 class="text">Validation</h1>

            <p class="text">
                We systematically compare the performance of the proposed LangBridge with baseline models across multiple evaluation metrics and highlight its unique advantage of enabling pre-training-free transfer between different large language models. Specifically, we explore two scenarios: <a href="#same_architecture"><strong>same-architecture transfer</strong></a> and <a href="#cross_architecture"><strong>cross-architecture transfer</strong></a>, to demonstrate the unique advantage of the proposed paradigm fully. 
            </p>

            <h3 id="same_architecture" class="text">Same-architecture Transfer</h3>
            <p class="text">
                We first examine the effectiveness of LangBridge in the context of same-architecture transfer. This setting is of great significance for efficiently scaling up model size, as it allows leveraging knowledge learned on smaller, more readily trainable models to improve the performance of larger ones. As shown in <a href="#tab:model_ensemble"><strong>Table 1</strong></a>, the LangBridge module, pre-trained or fine-tuned on a Qwen2-0.5B model, can be seamlessly transferred to Qwen2-7B and Qwen2.5-14B models without requiring computationally expensive re-pretraining.
                The results demonstrate that this transfer not only maintains strong performance but also, in several instances, exceeds the performance of the baseline models trained directly on the target LLM size.
            </p>

            <div id="tab:model_ensemble" style="display: flex; flex-direction: column; align-items: center;">
                <div class="table-container">
                    <table class="data-table">
                        <caption style="caption-side: top; text-align: left; margin-bottom: 10px;">
                            <strong>Table 1: Same-architecture transfer results.</strong>
                        </caption>
                        <thead>
                            <tr>
                                <th>SFT-LLM</th>
                                <th>Connector</th>
                                <th>GQA</th>
                                <th>TextVQA</th>
                                <th>MME</th>
                                <th>MMBench</th>
                                <th>MMVeT</th>
                                <th>POPE</th>
                                <th>SciQA</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Qwen2-7B</td>
                                <td>Qwen2-7B-Pretrain-MLPs</td>
                                <td>62.92</td>
                                <td>57.24</td>
                                <td>1938</td>
                                <td>72.7</td>
                                <td>35.5</td>
                                <td>87.8</td>
                                <td>79.44</td>
                            </tr>
                            <tr style="background-color: #e6f3ff;">
                                <td>-</td>
                                <td>Qwen2-0.5B-Pretrain-LangBridge</td>
                                <td>63.03 <span style="color: red;">(+0.17%)</span></td>
                                <td>57.25 <span style="color: red;">(+0.02%)</span></td>
                                <td>1886 <span style="color: blue;">(-2.68%)</span></td>
                                <td>71.7 <span style="color: blue;">(-1.38%)</span></td>
                                <td>34.1 <span style="color: blue;">(-3.94%)</span></td>
                                <td>88.2 <span style="color: red;">(+0.46%)</span></td>
                                <td>79.23 <span style="color: blue;">(-0.26%)</span></td>
                            </tr>
                            <tr style="background-color: #ffe6e6;">
                                <td>-</td>
                                <td>Qwen2-0.5B-SFT-LangBridge</td>
                                <td>63.15 <span style="color: red;">(+0.37%)</span></td>
                                <td>57.34 <span style="color: red;">(+0.17%)</span></td>
                                <td>1904 <span style="color: blue;">(-1.75%)</span></td>
                                <td>71.0 <span style="color: blue;">(-2.34%)</span></td>
                                <td>31.6 <span style="color: blue;">(-10.99%)</span></td>
                                <td>88.3 <span style="color: red;">(+0.57%)</span></td>
                                <td>79.25 <span style="color: blue;">(-0.24%)</span></td>
                            </tr>
                            <tr>
                                <td>Qwen2.5-7B</td>
                                <td>Qwen2.5-7B-Pretrain-MLPs</td>
                                <td>62.70</td>
                                <td>57.83</td>
                                <td>1939</td>
                                <td>73.8</td>
                                <td>34.8</td>
                                <td>88.4</td>
                                <td>79.11</td>
                            </tr>
                            <tr style="background-color: #e6f3ff;">
                                <td>-</td>
                                <td>Qwen2-0.5B-Pretrain-LangBridge</td>
                                <td>62.68 <span style="color: blue;">(-0.03%)</span></td>
                                <td>57.35 <span style="color: blue;">(-0.83%)</span></td>
                                <td>1871 <span style="color: blue;">(-3.51%)</span></td>
                                <td>72.3 <span style="color: blue;">(-2.03%)</span></td>
                                <td>34.7 <span style="color: blue;">(-0.29%)</span></td>
                                <td>88.1 <span style="color: blue;">(-0.34%)</span></td>
                                <td>81.23 <span style="color: red;">(+2.68%)</span></td>
                            </tr>
                            <tr style="background-color: #ffe6e6;">
                                <td>-</td>
                                <td>Qwen2-0.5B-SFT-LangBridge</td>
                                <td>62.69 <span style="color: blue;">(-0.02%)</span></td>
                                <td>57.94 <span style="color: red;">(+0.19%)</span></td>
                                <td>1915 <span style="color: blue;">(-1.24%)</span></td>
                                <td>72.7 <span style="color: blue;">(-1.49%)</span></td>
                                <td>35.6 <span style="color: red;">(+2.30%)</span></td>
                                <td>88.1 <span style="color: blue;">(-0.34%)</span></td>
                                <td>77.1 <span style="color: blue;">(-2.54%)</span></td>
                            </tr>
                            <tr>
                                <td>Qwen2.5-14B</td>
                                <td>Qwen2.5-14B-Pretrain-MLPs</td>
                                <td>63.71</td>
                                <td>61.32</td>
                                <td>2038</td>
                                <td>78.2</td>
                                <td>37.7</td>
                                <td>88.1</td>
                                <td>85.59</td>
                            </tr>
                            <tr style="background-color: #e6f3ff;">
                                <td>-</td>
                                <td>Qwen2-0.5B-Pretrain-LangBridge</td>
                                <td>63.75 <span style="color: red;">(+0.06%)</span></td>
                                <td>61.57 <span style="color: red;">(+0.41%)</span></td>
                                <td>1963 <span style="color: blue;">(-3.68%)</span></td>
                                <td>76.2 <span style="color: blue;">(-2.56%)</span></td>
                                <td>35.9 <span style="color: blue;">(-4.77%)</span></td>
                                <td>88.2 <span style="color: red;">(+0.11%)</span></td>
                                <td>84.74 <span style="color: blue;">(-0.99%)</span></td>
                            </tr>
                            <tr style="background-color: #ffe6e6;">
                                <td>-</td>
                                <td>Qwen2-0.5B-SFT-LangBridge</td>
                                <td>63.92 <span style="color: red;">(+0.33%)</span></td>
                                <td>62.02 <span style="color: red;">(+1.14%)</span></td>
                                <td>1990 <span style="color: blue;">(-2.36%)</span></td>
                                <td>77.4 <span style="color: blue;">(-1.02%)</span></td>
                                <td>38.4 <span style="color: red;">(+1.86%)</span></td>
                                <td>87.6 <span style="color: blue;">(-0.57%)</span></td>
                                <td>84.77 <span style="color: blue;">(-0.96%)</span></td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>

            <h3 id="cross_architecture" class="text">Cross-architecture Transfer</h3>
            <p class="text">
                Furthermore, we explore whether LangBridge demonstrates excellent performance in cross-architecture transfer scenarios (e.g., Qwen, LLaMA), specifically examining transfers from Qwen series models to LLaMA3 and vice versa. As shown in <strong><a href="#tab:cross_architecture">Table 2</a></strong>, LangBridge demonstrates a remarkable ability to maintain strong performance, retaining a high percentage of the baseline performance, exceeding 99% on many benchmarks. Moreover, the transfer from Qwen2-0.5B to Llama3-8B results in significant improvements, with performance gains observed across most benchmarks, especially MMVeT.
            </p>

            <p class="text">
                In the reverse direction, transferring from LLaMA3-8B to Qwen models (7B to 14B), LangBridge maintains robust performance, consistently achieving above 99% of the baseline scores across most benchmarks. In several cases, it even surpasses the baseline performance, such as when transferring to Qwen2.5-7B, where the LLaMA3-8B-SFT-LangBridge connector improves MMVeT (+8.33%) and POPE (+1.92%). Similarly, when applied to Qwen2.5-14B, the connector achieves gains in GQA (+1.19%) and MMVeT (+1.59%). These results demonstrate LangBridge's strong capability for cross-architecture transfer, effectively bridging different model families while maintaining or even enhancing performance across diverse multimodal tasks.
            </p>

            <div id="tab:cross_architecture" style="display: flex; flex-direction: column; align-items: center;">
                <div class="table-container">
                    <table class="data-table">
                        <caption style="caption-side: top; text-align: left; margin-bottom: 10px;">
                            <strong>Table 2: Cross-architecture transfer results.</strong>
                        </caption>
                        <thead>
                            <tr>
                                <th>SFT-LLM</th>
                                <th>Connector</th>
                                <th>GQA</th>
                                <th>TextVQA</th>
                                <th>MME</th>
                                <th>MMBench</th>
                                <th>MMVeT</th>
                                <th>POPE</th>
                                <th>SciQA</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>LLaMA3-8B</td>
                                <td>LLaMA3-8B-Pretrain-MLPs</td>
                                <td>63.24</td>
                                <td>55.66</td>
                                <td>1736</td>
                                <td>71.0</td>
                                <td>31.0</td>
                                <td>87.1</td>
                                <td>78.61</td>
                            </tr>
                            <tr style="background-color: #e6f3ff;">
                                <td>-</td>
                                <td>Qwen2-0.5B-Pretrain-LangBridge</td>
                                <td>64.14 <span style="color: red;">(+1.42%)</span></td>
                                <td>56.70 <span style="color: red;">(+1.87%)</span></td>
                                <td>1772 <span style="color: red;">(+2.07%)</span></td>
                                <td>70.1 <span style="color: blue;">(-1.27%)</span></td>
                                <td>34.0 <span style="color: red;">(+9.68%)</span></td>
                                <td>87.5 <span style="color: red;">(+0.46%)</span></td>
                                <td>78.78 <span style="color: red;">(+0.22%)</span></td>
                            </tr>
                            <tr style="background-color: #ffe6e6;">
                                <td>-</td>
                                <td>Qwen2-0.5B-SFT-LangBridge</td>
                                <td>63.96 <span style="color: red;">(+1.14%)</span></td>
                                <td>57.35 <span style="color: red;">(+3.04%)</span></td>
                                <td>1804 <span style="color: red;">(+3.92%)</span></td>
                                <td>69.4 <span style="color: blue;">(-2.25%)</span></td>
                                <td>32.7 <span style="color: red;">(+5.48%)</span></td>
                                <td>87.5 <span style="color: red;">(+0.46%)</span></td>
                                <td>78.33 <span style="color: blue;">(-0.36%)</span></td>
                            </tr>
                            <tr>
                                <td>Qwen2-7B</td>
                                <td>Qwen2-7B-Pretrain-MLPs</td>
                                <td>62.92</td>
                                <td>57.24</td>
                                <td>1938</td>
                                <td>72.7</td>
                                <td>35.5</td>
                                <td>87.8</td>
                                <td>79.44</td>
                            </tr>
                            <tr style="background-color: #e6f3ff;">
                                <td>-</td>
                                <td>LLaMA3-8B-Pretrain-LangBridge</td>
                                <td>62.90 <span style="color: blue;">(-0.03%)</span></td>
                                <td>57.23 <span style="color: blue;">(-0.02%)</span></td>
                                <td>1874 <span style="color: blue;">(-3.30%)</span></td>
                                <td>72.1 <span style="color: blue;">(-0.83%)</span></td>
                                <td>34.5 <span style="color: blue;">(-2.82%)</span></td>
                                <td>87.6 <span style="color: blue;">(-0.23%)</span></td>
                                <td>80.17 <span style="color: red;">(+0.92%)</span></td>
                            </tr>
                            <tr style="background-color: #ffe6e6;">
                                <td>-</td>
                                <td>LLaMA3-8B-SFT-LangBridge</td>
                                <td>62.77 <span style="color: blue;">(-0.24%)</span></td>
                                <td>57.08 <span style="color: blue;">(-0.28%)</span></td>
                                <td>1915 <span style="color: blue;">(-1.19%)</span></td>
                                <td>71.7 <span style="color: blue;">(-1.38%)</span></td>
                                <td>33.2 <span style="color: blue;">(-6.48%)</span></td>
                                <td>88.2 <span style="color: red;">(+0.46%)</span></td>
                                <td>78.83 <span style="color: blue;">(-0.77%)</span></td>
                            </tr>
                            <tr>
                                <td>Qwen2.5-7B</td>
                                <td>Qwen2.5-7B-Pretrain-MLPs</td>
                                <td>62.70</td>
                                <td>57.83</td>
                                <td>1939</td>
                                <td>73.8</td>
                                <td>34.8</td>
                                <td>88.4</td>
                                <td>79.11</td>
                            </tr>
                            <tr style="background-color: #e6f3ff;">
                                <td>-</td>
                                <td>LLaMA3-8B-Pretrain-LangBridge</td>
                                <td>62.72 <span style="color: red;">(+0.03%)</span></td>
                                <td>57.78 <span style="color: blue;">(-0.09%)</span></td>
                                <td>1878 <span style="color: blue;">(-3.15%)</span></td>
                                <td>71.9 <span style="color: blue;">(-2.57%)</span></td>
                                <td>32.3 <span style="color: blue;">(-7.18%)</span></td>
                                <td>88.4 <span style="color: red;">(+0.00%)</span></td>
                                <td>80.59 <span style="color: red;">(+1.87%)</span></td>
                            </tr>
                            <tr style="background-color: #ffe6e6;">
                                <td>-</td>
                                <td>LLaMA3-8B-SFT-LangBridge</td>
                                <td>63.03 <span style="color: red;">(+0.53%)</span></td>
                                <td>57.72 <span style="color: blue;">(-0.19%)</span></td>
                                <td>1909 <span style="color: blue;">(-1.55%)</span></td>
                                <td>71.1 <span style="color: blue;">(-3.66%)</span></td>
                                <td>37.7 <span style="color: red;">(+8.33%)</span></td>
                                <td>90.1 <span style="color: red;">(+1.92%)</span></td>
                                <td>79.84 <span style="color: red;">(+0.92%)</span></td>
                            </tr>
                            <tr>
                                <td>Qwen2.5-14B</td>
                                <td>Qwen2.5-14B-Pretrain-MLPs</td>
                                <td>63.71</td>
                                <td>61.32</td>
                                <td>2038</td>
                                <td>78.2</td>
                                <td>37.7</td>
                                <td>88.1</td>
                                <td>85.59</td>
                            </tr>
                            <tr style="background-color: #e6f3ff;">
                                <td>-</td>
                                <td>LLaMA3-8B-Pretrain-LangBridge</td>
                                <td>63.88 <span style="color: red;">(+0.27%)</span></td>
                                <td>61.39 <span style="color: blue;">(-0.11%)</span></td>
                                <td>2005 <span style="color: blue;">(-1.62%)</span></td>
                                <td>77.1 <span style="color: blue;">(-1.41%)</span></td>
                                <td>36.9 <span style="color: blue;">(-2.12%)</span></td>
                                <td>87.9 <span style="color: blue;">(-0.23%)</span></td>
                                <td>85.03 <span style="color: blue;">(-0.65%)</span></td>
                            </tr>
                            <tr style="background-color: #ffe6e6;">
                                <td>-</td>
                                <td>LLaMA3-8B-SFT-LangBridge</td>
                                <td>64.47 <span style="color: red;">(+1.19%)</span></td>
                                <td>61.35 <span style="color: blue;">(-0.05%)</span></td>
                                <td>1980 <span style="color: blue;">(-2.85%)</span></td>
                                <td>76.3 <span style="color: blue;">(-2.43%)</span></td>
                                <td>38.3 <span style="color: red;">(+1.59%)</span></td>
                                <td>87.8 <span style="color: blue;">(-0.34%)</span></td>
                                <td>84.23 <span style="color: blue;">(-1.59%)</span></td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
        </div>
        

        </d-article>
        <d-appendix>
            <h3>BibTeX</h3>
            <p class="bibtex">
                @article{kang2024how,<br>
                &nbsp;&nbsp;title={How Far is Video Generation from World Model? -- A Physical Law Perspective},<br>
                &nbsp;&nbsp;author={Kang, Bingyi and Yue, Yang and Lu, Rui and Lin, Zhijie and Zhao, Yang, and Wang, Kaixin and Huang, Gao and Feng, Jiashi},<br>
                &nbsp;&nbsp;journal={arXiv preprint arXiv:2411.02385},<br>
                &nbsp;&nbsp;year={2024}<br>
                }
            </p>

            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
        </d-appendix>   
        <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
        <d-bibliography src="bibliography.bib"></d-bibliography>
        <script src="./static/js/nav-bar.js"></script>
        <script>
            function toggleContent() {
                const content = document.getElementById("twitter_container");
                // ÂàáÊç¢displayÂ±ûÊÄß
                if (content.style.display === "none") {
                    content.style.display = "block"; // Â±ïÂºÄ
                } else {
                    content.style.display = "none"; // ÊäòÂè†
                }
            }
        </script>
    </body>
</html>
